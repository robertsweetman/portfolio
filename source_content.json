{
  "2": "# Stakeholder Engegement & crafting a solution <!-- 500 words -->\r\n\r\n## Key stakeholders and requirements\r\n<!--\r\nIdentify key stakeholders, then gather and prioritise requirements\r\n-->\r\nKey stakeholders are the business leaders responsible for the sales effort in Ireland, the development team who build and deploy internal applications and the support team responsible for ongoing management of their applications.\r\n\r\n- **Sales Team**: Focus on usability, visibility, deadline management (core business needs) and winning tenders\r\n- **Development Team**: Emphasis on security, compliance, and authentication (protecting sensitive tender data)\r\n- **Support Team**: Priority on maintainability, alarms, user management, and documentation (keeping the system running smoothly)\r\n\r\n<!--\r\n| Stakeholder | Requirements                        | Priority |\r\n|:------------|:-----------------------------------:|---------:|\r\n| Sales Team  | Easy to use, access and update      | High     |\r\n|             | Integrated with Sales Process/CRM   | High     |\r\n|             | Deadline tracking & reminders       | High     |\r\n|             | Tender and Response PDF downloads   | High     |\r\n|             | Search & filtering capabilities *   | Medium   |\r\n|             | Win/loss analysis *                 | Medium   |\r\n|             | Reporting & dashboards              | High     |\r\n|             | Mobile accessibility                | Low      |\r\n| Dev Team    | Scalable                            | Medium   |\r\n|             | Familiar Programming Language       | High     |\r\n|             | Data security & access control      | High     |\r\n|             | GDPR compliance                     | High     |\r\n|             | Backup & disaster recovery          | High     |\r\n|             | API design for integrations         | Medium   |\r\n|             | Database performance optimization   | Medium   |\r\n|             | CI/CD pipeline                      | Medium   |\r\n|             | Infrastructure cost management      | Medium   |\r\n|             | Authentication (SSO/MFA)            | High     |\r\n|             | Audit trails                        | High     |\r\n| Support     | Maintainable                        | High     |\r\n|             | Has logging                         | Medium   |\r\n|             | Alarms when it fails                | High     |\r\n|             | User management & permissions       | High     |\r\n|             | Clear error messages                | Medium   |\r\n|             | Training documentation              | High     |\r\n|             | SLA monitoring                      | Medium   |\r\n|             | Troubleshooting tools               | Medium   |\r\n|             | Backup/restore procedures           | High     |\r\n|             | Usage analytics                     | Low      |\r\n|             | Change management process           | Medium   |\r\n-->\r\n\r\n![Requirements](./images/project_2/Requirements.png)\r\nFigure 3: Business and Technical Requirements\r\n\r\n## Research\r\n\r\n### Database Options\r\n\r\nBy taking the characteristics of the data into account we can rule out certain database types.\r\n\r\nIt's not streaming, unstructured data so no-sql data types aren't really valuable here. While you could consider each tender record to be a \"document\" there really isn't the update frequency, scalability issues or fault tolerance requirements because we're dealing with less than 3000 records.\r\n\r\nWe can use a traditional SQL scheme approach because the structure of the data is well understood, it doesn't change often and we can use traditional backup methods rather than having to worry about scaling or data-replication.\r\n\r\n- Database schema for tables is well understood\r\n- Database schema doesn't change frequently\r\n- Updates (writes) are infrequent\r\n- Reads are infrequent\r\n- Total data volume is not large (3000 - 5000 records max)  \r\n\r\nWhat is possibly open for debate is how to load/update and interact with the database. Should it be in the cloud? Which one? Should it be hosted on a server or as a service? Which particular flavour of SQL should we use? Postgresql? Microsoft SQL? We can also use several Azure hosted options (Microsoft.com, 2025)\r\n\r\nFrom talking to Technical & Support stakeholders we do know it needs to interact with Microsoft services (Dynamics 365, Fabric) and potentially run in Azure.\r\n\r\nWe can look at options with a weighting, including a noSQL option for comparison\r\n\r\n| Criteria | Weight | Azure SQL | PostgreSQL | Cosmos DB |\r\n|----------|--------|-----------|------------|-----------|\r\n| Sales (M365) Integration | 25% | 8/10 | 8/10 | 7/10 |\r\n| Team Familiarity | 20% | 8/10 | 9/10 | 4/10 |\r\n| Security & Compliance | 20% | 9/10 | 9/10 | 8/10 |\r\n| Cost (3-5k records) | 15% | 6/10 | 9/10 | 5/10 |\r\n| Scalability | 10% | 8/10 | 8/10 | 10/10 |\r\n| Maintainability | 10% | 9/10 | 8/10 | 7/10 |\r\n| **Weighted Score** | | **8.0** | **8.6** | **6.6** |\r\n\r\nFigure 4: SWOT Analysis with Weighting\r\n\r\nAnalysis shows that Azure hosted PostgreSQL is the preferred option, especially because we can also develop the entire solution using locally hosted PostgreSQL to save time initially.\r\n\r\n<!--\r\nUsing relevant data analysis techniques\r\n  - analyse stakeholder feedback\r\n  - research and compare potential organisational data solutions\r\n\r\nResearch and compare potential organisational data solutions to generate ideas for developing a data solution\r\n\r\nNOTES:\r\n- weighting vs requirements\r\n- SWOT analyse\r\n- ??\r\n\r\n-->\r\n\r\n## Stakeholder feedback\r\n\r\nThe commerical team needs to focus primarily on business outcomes and getting feedback on the workflow is far easier when it's explained visually:\r\n\r\n```mermaid\r\nflowchart TD\r\n    Start([Daily Tender Check]) --> Gather[Gather Tender Records<br/>from eTenders.gov.ie]\r\n    Gather --> PDF[Extract PDF Content<br/>& Metadata]\r\n    PDF --> CPV{Check CPV Codes<br/>Relevant?}\r\n    \r\n    CPV -->|No| Archive[Archive as<br/>Not Relevant]\r\n    CPV -->|Yes| AI[AI/ML Analysis<br/>Score & Recommend]\r\n    \r\n    AI --> Decision{AI Recommendation<br/>Score > Threshold?}\r\n    Decision -->|No| Archive\r\n    Decision -->|Yes| Alert[Alert Sales Team<br/>via Email/Dashboard]\r\n    \r\n    Alert --> Review[Sales Team Review<br/>in Dashboard]\r\n    Review --> Pursue{Pursue<br/>Tender?}\r\n    \r\n    Pursue -->|No| Declined[Mark as Declined<br/>+ Reason]\r\n    Pursue -->|Yes| BidPrep[Prepare Bid Response]\r\n    \r\n    BidPrep --> UploadDoc[Upload Bid Documents<br/>to Database]\r\n    UploadDoc --> Submit[Submit Tender Response]\r\n    \r\n    Submit --> Track[Track Submission<br/>+ Deadline]\r\n    Track --> Outcome[Await Tender Outcome]\r\n    \r\n    Outcome --> Result{Tender<br/>Result?}\r\n    Result -->|Won| Won[Update: Tender WON<br/>+ Contract Value]\r\n    Result -->|Lost| Lost[Update: Tender LOST<br/>+ Competitor Info]\r\n    \r\n    Won --> Analysis[Analysis Dashboard<br/>Win/Loss Analytics]\r\n    Lost --> Analysis\r\n    Declined --> Analysis\r\n    Archive --> Analysis\r\n    \r\n    Analysis --> Feedback[Feedback Loop:<br/>Improve AI Model]\r\n    Feedback --> Templates[Generate Response<br/>Templates from Wins]\r\n    \r\n    Templates -.->|Improve Future Bids| BidPrep\r\n    Feedback -.->|Retrain Model| AI\r\n    \r\n    Analysis --> Reports[Generate Reports:<br/>- Success Rate by CPV<br/>- Competitor Analysis<br/>- Response Template Library]\r\n    \r\n    Reports --> Start\r\n\r\n    style Start fill:#e1f5e1\r\n    style Alert fill:#fff3cd\r\n    style Won fill:#d4edda\r\n    style Lost fill:#f8d7da\r\n    style Analysis fill:#d1ecf1\r\n    style Feedback fill:#cce5ff\r\n    style Templates fill:#d4edda\r\n```\r\n\r\nFigure 5: Business Process Flow\r\n\r\nWe should aim to get technical stakeholder buy in through focusing on data-flow, using standardised tools, ease of support, security, logging and maintainability.\r\n\r\n```mermaid\r\ngraph TB\r\n    subgraph \"External Systems\"\r\n        ETenders[eTenders.gov.ie<br/>API]\r\n        M365[Microsoft 365<br/>Dynamics 365]\r\n        Email[Email Service<br/>SMTP/Graph API]\r\n    end\r\n    \r\n    subgraph \"Azure Cloud Platform\"\r\n        subgraph \"Application Layer\"\r\n            WebApp[Web Dashboard<br/>ASP.NET/Blazor]\r\n            API[REST API<br/>Authenticated]\r\n            Functions[Azure Functions<br/>Scheduled Tasks]\r\n        end\r\n        \r\n        subgraph \"Data Layer\"\r\n            PostgreSQL[(Azure PostgreSQL<br/>Managed Service)]\r\n            BlobStorage[Blob Storage<br/>PDF Documents]\r\n            AIService[Azure OpenAI<br/>ML Analysis]\r\n        end\r\n        \r\n        subgraph \"Security & Monitoring\"\r\n            AAD[Azure AD<br/>SSO/MFA]\r\n            KeyVault[Key Vault<br/>Secrets Management]\r\n            Monitor[Application Insights<br/>Logging & Alerts]\r\n            Backup[Automated Backups<br/>Point-in-Time Recovery]\r\n        end\r\n    end\r\n    \r\n    subgraph \"CI/CD Pipeline\"\r\n        GitHub[GitHub Repo]\r\n        Actions[GitHub Actions]\r\n        Deploy[Azure DevOps]\r\n    end\r\n    \r\n    ETenders -->|HTTPS| Functions\r\n    Functions -->|Write| PostgreSQL\r\n    Functions -->|Store PDFs| BlobStorage\r\n    Functions -->|AI Analysis| AIService\r\n    \r\n    WebApp -->|Auth| AAD\r\n    API -->|Auth| AAD\r\n    WebApp -->|Query| API\r\n    API -->|Read/Write| PostgreSQL\r\n    API -->|Retrieve PDFs| BlobStorage\r\n    \r\n    PostgreSQL -->|Integration| M365\r\n    API -->|Alerts| Email\r\n    \r\n    API -->|Secrets| KeyVault\r\n    Functions -->|Secrets| KeyVault\r\n    \r\n    WebApp -->|Logs| Monitor\r\n    API -->|Logs| Monitor\r\n    Functions -->|Logs| Monitor\r\n    \r\n    PostgreSQL -->|Daily| Backup\r\n    \r\n    GitHub -->|Trigger| Actions\r\n    Actions -->|Build & Test| Deploy\r\n    Deploy -->|Deploy| WebApp\r\n    Deploy -->|Deploy| Functions\r\n    \r\n    style AAD fill:#0078d4\r\n    style Monitor fill:#ff6b6b\r\n    style KeyVault fill:#ffd93d\r\n    style Backup fill:#6bcf7f\r\n    style PostgreSQL fill:#336791\r\n```\r\n\r\nFigure 6: Technical Architecture\r\n\r\nWe're going to take the Python functions used to develop the proof of concept and re-work them into Azure Functions.\r\n\r\nWe can also use the PostgreSQL database as a back-end for Microsoft Fabric (abnarain, 2025) which will integrate with incoming email updates about tender awards as well as updates from the Sales Team.\r\n\r\n<!--\r\nGenerate Ideas for developing a data solution, or update to data architecture\r\nUse data viz tools to present insights and gather initial stakeholder feedback\r\n-->\r\n\r\n<!--\r\n\r\nDemostrate a funcamental understanding of how to implement a data analytics and/or engineering solution to measure and realise value.\r\n\r\nRUBRIC - B\r\n\r\nDesigns and implements a data solution that meets business requirements. Focuses on scalability, performance optimization, security, and regulatory compliance. Conducts comprehensive testing and automation with scheduling.\r\n\r\nRUBRIC - A\r\n\r\nDesigns and implements a sophisticated data solution that meets business requirements. Focuses on scalability, performance optimization, security, and regulatory compliance. Conducts comprehensive testing and automation with scheduling. Includes flexible design for future changes and adherence to best practices.\r\n-->\r\n",
  "4": "# Proof of Concept <!-- 500 -->\r\n\r\nThe revised architecture can be roughed out in a simple proof of concept using Python to capture the eTenders data from the site, parsing the PDF content and have AI split the content of the PDF into additional fields within the pdf_content table.\r\n\r\nBefore the advent of AI we'd have used Regex for this last step but now that LLM's can output structured data reliably using a locally hosted LLM (possibly running in a container) this is basically free and easy to implement.\r\n\r\nFollowing software development best practice around separation of concerns we can have Python fuctions that carry out discrete steps: -\r\n\r\n1. Scrape the data\r\n2. Parse it into the correct type\r\n3. Retreive the PDF content\r\n4. Parse the PDF content using a local LLM\r\n5. Analyse the number of CPVs in the tender\r\n6. Have a local LLM analyse the tender\r\n7. Output the content, store or send it 'somewhere' - options are csv, json or postgres\r\n\r\nIf we don't know \"where\" to send the data at this point we could have multiple functions to do step 7 and just vary the \"write\" parts at the end.\r\n\r\n```mermaid\r\nflowchart TD\r\n    A[eTenders Website] -->|Scrape Data| B[Raw Data]\r\n    B -->|Parse & Type Coercion| C[Structured Data]\r\n    C -->|Load to Database| D[(ETENDERS_CORE)]\r\n    D -->|Extract PDF URLs| E[Download PDFs]\r\n    E -->|Parse PDF Content| F[Local LLM Processing]\r\n    F -->|Store PDF Data| G[(ETENDERS_PDF)]\r\n    F -->|Analyze CPV Codes| H[(CPV_CHECKER)]\r\n    F -->|Generate Bid Recommendation| I[(BID_ANALYSIS)]\r\n    \r\n    J[Sales Team] -->|Progress Updates| K[(SALES_UPDATES)]\r\n    L[Sales Team] -->|Bid Submissions| M[(BID_SUBMISSION)]\r\n    N[Irish Government Emails] -->|Tender Status<br/>Won/Lost/Awarded| O[(TENDER_NOTIFICATION)]\r\n    \r\n    D -.->|Links via resource_id| G\r\n    D -.->|Links via resource_id| H\r\n    D -.->|Links via resource_id| I\r\n    D -.->|Links via resource_id| K\r\n    D -.->|Links via resource_id| M\r\n    D -.->|Links via resource_id| O\r\n    \r\n    style A fill:#e1f5ff\r\n    style F fill:#ffe1f5\r\n    style D fill:#e1ffe1\r\n    style G fill:#e1ffe1\r\n    style H fill:#e1ffe1\r\n    style I fill:#e1ffe1\r\n    style K fill:#e1ffe1\r\n    style M fill:#e1ffe1\r\n    style O fill:#e1ffe1\r\n    style J fill:#fff4e1\r\n    style L fill:#fff4e1\r\n    style N fill:#fff4e1\r\n```\r\n\r\nFigure 8: Data Pipeline Flow Diagram\r\n\r\nThe technical aspect of implementing these steps is NOT complicated and doesn't require the use of third party tools. In fact, using Python allows us to really tune the solution to our needs.\r\n\r\n## Testing\r\n\r\nAnother advantage of writing the data pipeline ourselves is that we can create tests to comprehensively handle each step in the process.\r\n\r\n![Test Results](images/test_results.png)\r\nFigure 9: Test Results\r\n\r\nThis encompasses the following: -\r\n\r\n- Data structure checks\r\n- Type coercion for dates & currencies\r\n- PDF downloads & validation\r\n- Getting the CPV codes\r\n- Validating the DB schema\r\n- Data integrity and business rules\r\n\r\nBeing able to check & run these tests on a consistent basis could easily be added to any deployment pipeline too.\r\n\r\nThe most important (initial) tests make sure that the eTenders data format that's being scraped hasn't been changed by the source. The Irish Govt in this case.\r\n\r\n## Logging & Debugging\r\n\r\nWe can make sure that each step has proper logging which can be turned on as needed via the `--enable-logging` flag.\r\n\r\nThe entire data pipeline logs outputs to a file, in this case see ![etenders_20251128_131354.log](https://github.com/robertsweetman/module_4/blob/main/python/etenders_20251128_131354.log) where we can see issues arrise and take measures to fix the pipeline directly.\r\n\r\nBuilding our own data pipeline in code means we can respond flexibly and quickly to issues that arrise.\r\n\r\n### PDF Extraction Errors\r\n\r\nAbout 30% of errors (not records!) are PDF extraction errors.\r\n\r\n```log\r\n2025-11-28 14:39:26,109 - pdf_parser - ERROR - Error extracting text from PDF https://www.etenders.gov.ie/epps/cft/downloadNoticeForAdvSearch.do?resourceId=6965752: No /Root object! - Is this really a PDF?\r\nTraceback (most recent call last):\r\n  File \"/Users/robert.sweetman/Documents/GitHub/module_4/python/pdf_parser.py\", line 54, in extract_pdf_text\r\n    text = extract_text(pdf_file)\r\n  File \"/Users/robert.sweetman/Documents/GitHub/module_4/python/venv/lib/python3.9/site-packages/pdfminer/high_level.py\", line 177, in extract_text\r\n    for page in PDFPage.get_pages(\r\n  File \"/Users/robert.sweetman/Documents/GitHub/module_4/python/venv/lib/python3.9/site-packages/pdfminer/pdfpage.py\", line 159, in get_pages\r\n    doc = PDFDocument(parser, password=password, caching=caching)\r\n  File \"/Users/robert.sweetman/Documents/GitHub/module_4/python/venv/lib/python3.9/site-packages/pdfminer/pdfdocument.py\", line 752, in __init__\r\n    raise PDFSyntaxError(\"No /Root object! - Is this really a PDF?\")\r\npdfminer.pdfparser.PDFSyntaxError: No /Root object! - Is this really a PDF?\r\n```\r\n\r\nFigure 10: PDF Extraction Errors\r\n\r\nSome URLs that are in the eTenders data are possibly HTML pages or corrupted. Now that's clear we can add defensive code fixes to the pipeline and improve the logging further.\r\n\r\n```python\r\ndef download_and_validate_pdf(url):\r\n    try:\r\n        response = requests.get(url, timeout=30)\r\n        \r\n        # Check Content-Type header first\r\n        content_type = response.headers.get('Content-Type', '')\r\n        if 'application/pdf' not in content_type:\r\n            log_error(f\"Not a PDF (Content-Type: {content_type})\")\r\n            return None\r\n            \r\n        # Validate PDF magic bytes\r\n        if not response.content.startswith(b'%PDF'):\r\n            log_error(f\"Invalid PDF format\")\r\n            return None\r\n            \r\n        return response.content\r\n    except Exception as e:\r\n        log_error(f\"Download failed: {e}\")\r\n        return None\r\n```\r\n\r\nFigure 11: Python fix for PDF Extraction Errors\r\n\r\n### Server Errors\r\n\r\nAbout 35% of errors involve the server not responding to certain requests.\r\n\r\n```log\r\n2025-11-28 14:17:29,640 - pdf_parser - ERROR - Failed to download PDF from https://www.etenders.gov.ie/epps/cft/downloadNoticeForAdvSearch.do?resourceId=7012322: 500 Server Error: Internal Server Error for url: https://www.etenders.gov.ie/epps/cft/downloadNoticeForAdvSearch.do?resourceId=7012322\r\n```\r\n\r\nFigure 12: Server Errors\r\n\r\nWe can add some retry logic to the pipeline to fix this.\r\n\r\n```python\r\nfrom tenacity import retry, stop_after_attempt, wait_exponential\r\n\r\n@retry(\r\n    stop=stop_after_attempt(3),\r\n    wait=wait_exponential(multiplier=1, min=4, max=60)\r\n)\r\ndef download_with_retry(url):\r\n    response = requests.get(url, timeout=30)\r\n    response.raise_for_status()\r\n    return response.content\r\n```\r\n\r\nFigure 13: Python Fix for Server Errors\r\n\r\n### LLM JSON parsing errors\r\n\r\nThe LLM can produce invalid or incomplete JSON\r\n\r\n```log\r\n2025-11-28 14:27:26,769 - pdf_parser - ERROR - JSON Parse Error at line 1, column 202: Expecting ',' delimiter\r\n```\r\n\r\nFigure 14: JSON Parsing Errors\r\n\r\nAgain we can add some defensive python coding (REF: defensive coding) to address this by attempting to repair anything that contains this error\r\n\r\n```python\r\ndef parse_with_llm_robust(text):\r\n    prompt = \"\"\"Extract structured data as valid JSON. \r\n    IMPORTANT: Output ONLY complete, valid JSON with all closing braces.\r\n    \r\n    Required fields:\r\n    - procedure_id (string)\r\n    - title (string)\r\n    - buyer_name (string)\r\n    - buyer_country (string)\r\n    - estimated_value (number)\r\n    \r\n    Text to parse:\r\n    {text}\r\n    \"\"\"\r\n    \r\n    try:\r\n        response = ollama.generate(\r\n            model='llama2',\r\n            prompt=prompt.format(text=text[:4000]),  # Limit input\r\n            format='json'  # Force JSON output\r\n        )\r\n        \r\n        # Validate JSON before returning\r\n        parsed = json.loads(response['response'])\r\n        return parsed\r\n        \r\n    except json.JSONDecodeError as e:\r\n        # Try to salvage partial JSON\r\n        log_error(f\"JSON parse error: {e}\")\r\n        return attempt_json_repair(response['response'])\r\n```\r\n\r\nFigure 15: Python fix for JSON Parsing\r\n\r\n### Timeout Errors\r\n\r\nWe can see about 10% of errors are due to timeouts getting larger PDF's\r\n\r\n```log\r\n2025-11-28 14:11:49,230 - pdf_parser - ERROR - Error parsing with Ollama: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=90)\r\nTraceback (most recent call last):\r\n  File \"/Users/robert.sweetman/Documents/GitHub/module_4/python/venv/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 468, in _make_request\r\n    six.raise_from(e, None)\r\n  File \"<string>\", line 3, in raise_from\r\n  File \"/Users/robert.sweetman/Documents/GitHub/module_4/python/venv/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 463, in _make_request\r\n    httplib_response = conn.getresponse()\r\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py\", line 1349, in getresponse\r\n    response.begin()\r\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py\", line 316, in begin\r\n    version, status, reason = self._read_status()\r\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/http/client.py\", line 277, in _read_status\r\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\r\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/socket.py\", line 704, in readinto\r\n    return self._sock.recv_into(b)\r\nsocket.timeout: timed out\r\n```\r\n\r\nFigure 16: Timeout Errors\r\n\r\nSo we can add a retry queue to address these problems.\r\n\r\n```python\r\nclass ErrorHandler:\r\n    RETRYABLE = ['500', 'timeout', 'connection']\r\n    SKIP = ['No /Root object', 'JSON Parse Error']\r\n    \r\n    def handle_error(self, error, url):\r\n        if any(e in str(error) for e in self.RETRYABLE):\r\n            # Add to retry queue with backoff\r\n            self.retry_queue.add(url, delay=calculate_backoff())\r\n        else:\r\n            # Log and skip\r\n            self.failed_urls.add(url)\r\n            log_to_csv(url, error, 'SKIPPED')\r\n```\r\n\r\nFigure 17: Python fix for Timeout Errors\r\n\r\n<!--\r\nDesign, implement and debug a data product\r\n\r\nRUBRIC - B\r\n\r\nDevelop a comprehensive data product meeting business requirements, with focus on scalability, performance, security and compliance. Conducts thorough testing and logging\r\n\r\nRUBRIC - A\r\n\r\nDevelops a hightly adaptable, scalable data product exceeding current business needs. Implements a solution with optimized performance and proactive security measures. Provides actionable insights driving business value. Evaluates the use of automation and scheduling\r\n-->\r\n",
  "1": "# Data Engineering Project Evaluation Report\r\n<!-- 1200 words -->\r\n\r\nThe eTenders application was initially designed to eliminate the time wasted reading through 50 emails per day and uses AI to discern which tenders should be responded to.\r\n\r\n## Data Management\r\n\r\n<!-- The data load of eTenders is currently carried out in a pretty naive fashion. -->\r\n\r\nThe initial project scope didn't really encompass database design, integration, analysis or ongoing application support.\r\n\r\nStoring the eTenders data as rows and then reporting against their status within the processing pipeline was the only consideration in the beginning.\r\n\r\nThere was no integration with other data sources, the Postgresql database was hosted in AWS RDS (Amazon Web Services, Inc., 2019) to be accessible from the Rust AWS Lambda code and backup provision was minimal.\r\n\r\nHowever to reduce costs it is still open to the web (via username and password) since putting it behind an AWS Nat Gateway (AWS, 2020) to increase security costs an additional £30 per month.\r\n\r\nIt serves mainly as a state storage device to answer the following questions:\r\n\r\n1. Which tenders have already been looked at\r\n2. Where is a particular tender record in the AWS Lambda pipeline process\r\n3. Which tenders have we suggested to the business (via email) should be looked into further\r\n\r\nThe database of tender records isn't the 'product' in this case. The 'respond to this tender' emails are currently the only product or business value resulting from all this work.\r\n\r\n### Database table design\r\n\r\nThe existing application uses AWS Lamdba's written in Rust so from a basic data-gathering perspective we can make use of Rust's struct objects. Rust structs (doc.rust-lang.org, n.d.) are essentially custom objects which are the 'record' being moved and modified through the ETL pipeline.\r\n\r\n![TenderRecordRaw](images/TenderRecordRaw.png)\r\n\r\nFigure 1: Raw Tender Record\r\n\r\nInitially everything is collected as a String type and then parsed into a more appropriate type.\r\n\r\n![TenderRecord](images/TenderRecord.png)\r\n\r\nFigure 2: Transformed Tender Record\r\n\r\nThis TenderRecord struct is the base object which is either passed between Lambda's or written to the database. It's not a critical consideration but Rust as a language can perform these sorts of data type changes very quickly. Rust is faster than other languages, with the possible exception of C/C+ (Vietnam Software Outsourcing - MOR Software, 2016).\r\n\r\n#### Tender Records Table\r\n\r\nEach record is stored in a single row, like an excel spreadsheet. There is a unique resource_id per tender so we can use this as a key for \"something\" but not very much consideration has been given to how to use this data beyond the Machine Learning (ML) training that was carried out initially. (robertsweetman, 2025)\r\n\r\nIn a prior ML training phase, once the eTenders table contained enough records, a percentage were manually labelled as 'tenders we should bid on' and then basic linear regression with tokenisation was run against this training data.\r\n\r\nBeing able to gain other insights from the data wasn't really considered. It's purely been used to reduce Sales admin time & to try to avoid missing any tenders which should be looked at by a human.\r\n\r\n#### PDF Content Table\r\n\r\nIn most cases each tender has an accompanying PDF that contains more information about the bid process for that tender.\r\n\r\nThese PDF's are not always comprehensive but they do supply the AI analysis step with a lot of valuable context to help decide whether to bid on something or not.\r\n\r\nHowever, from a database point of view, the entire PDF is being stored as a long text string in an accompanying pdf_content column which is linked to the main eTenders table via the resource_id key.\r\n\r\n## Key Findings\r\n\r\nFrom talking to stakeholders there has been some very clear feedback.\r\n\r\nReducing the daily tender analysis time has benefited the sales team but the strictly linear workflow (get, parse, analyse, alert) doesn't really leverage the database to deliver further business value beyond automating this one process.\r\n\r\nThe business value is actually limited and doesn't justify the application maintenance and support requirements.\r\n\r\nThis is due to technical challenges in the way this proof-of-concept (POC) has been delivered:\r\n\r\n- Runs in AWS, not Azure\r\n  - Version 1's infra runs in Azure so this is a general blocker to wider adoption\r\n  - The support team are not set up to support AWS hosted apps\r\n- Uses Rust for Lambda's\r\n  - Developers aren't familiar with Rust and can't support it properly\r\n- The Postgresql database is still open to the internet for technical and cost reasons\r\n  - This is a significant security issue\r\n\r\nThe current implementation blocks further integrations with other business processes and tools, both from a commercial and technical point of view.\r\n\r\n<!--\r\nHighlight key findings and provide actional recommendations\r\n-->\r\n\r\n<!--\r\nEvaluate the current state of data management at your organisation, including data source integration, storage, quality, compliance with GDPR/HIPAA, security, and tool effectiveness.\r\n-->\r\n\r\n## Needs Analysis\r\n<!--\r\nDetail a comprehensive needs analysis highlighting data related needs and pain points\r\n-->\r\n### Business Needs\r\n\r\nThe organisation needs to place the tender requests firmly within the whole bid response process loop. All the current solution does is use AI to help guard against losing the signal within the noise of up to 50 requests per day. It doesn't really add value beyond that.\r\n\r\nHow can we capture the tender requests, filter out the ones we (as an IT Solutions Consultancy) are not interested in and tie the success or failure of a particular bid back to the original tender request?\r\n\r\nThis requires a much more robust and holistic data solution which can be connected to other parts of the sales effort.\r\n\r\nWe could update the database with won and lost tenders to help avoid repeating mistakes associated with losing bids. Another example might be updating the bid database with notifications from the Irish Government about who won bids to understand more about our competitors.\r\n\r\nIf we append what we send in response to tenders that might even allow us to leverage AI to create bid response templates, improve our ML tender analysis solution with actual \"won bid\" data and be more confident anything our ML/AI highlights will net a positive return on the time investment to respond to a tender request.\r\n\r\n### Technical Needs\r\n<!-- is it POC or PoC -->\r\nThe intial POC is hard to manage because it's in both an unfamiliar language and the incorrect cloud environment for the business.\r\n\r\nIf we are going to host a more robust solution in the cloud then it at least needs to be in Microsoft's Azure.\r\n\r\nOur database also needs to be connectable to the wider 'sales process' environment within the organisation. It's no good lobbing this into another proprietary tool that can't connect to.\r\n\r\n<!--\r\n\r\nCritically evaluate the design and implementation of organisational data architecture against business initiatives\r\n\r\nRUBRIC - B\r\n\r\nAnalyses organisational data architecture, focusing on its alignment with business initiatives. Provides clear and actionable recommendations for improvement.\r\n\r\nRUBRIC - A\r\n\r\nCritically evaluates organisational data architecture against business initiatives. Provides a comprehensive and strategic set of recommendations for improvement, supported by clear explanations and persuasive rationale.\r\n-->\r\n\r\n<!--\r\n# Milestone 1\r\n  - Complete a detailed evaluation and restructure proposal for an existing database architecture, focusing on ACID principles and stakeholder requirements.\r\n\r\n## Data Architecture Analysis\r\n  - Map out the existing data sources and describe the relationships between them.\r\n  - Evaluate the alignment of the current architecture with ACID principles, scalability, and performance.\r\n  - Identify pain points related to the data architecture, focusing on areas affecting data quality and accessibility.\r\n## Data Integration Challenges and Solution Proposal\r\n  - Discuss integration challenges (data quality, compliance, security) and their impacts on business operations.\r\n  - Develop a restructuring proposal, prioritizing stakeholder needs and outlining improvements in data flow and quality.\r\n  - Include a brief comparison of two potential architectural solutions or tools, with a focus on security and compliance.\r\n\r\nIMPORTANT: Create an ERD in lucidchart\r\n-->\r\n\r\n<!-- \r\nVERY IMPORTANT POINTS FROM REVIEW WITH JOE\r\n\r\n1. Where possibe EVALUATE choices against a set of criteria\r\n - explain\r\n - analyse\r\n - pros/cons\r\n - EVALUATE\r\n\r\n2. LO3 is a TRAP \r\n - design, implement, DEBUG (examples of how to debug), automation and scheduling \r\n\r\n3. Look at EVERY word in the rubric and make sure it's being covered somehow.\r\n-->",
  "3": "# Data Product Development <!-- 1000 words -->\r\n\r\n<!-- Understand the specific business problem to be addressed -->\r\n\r\nWe want to build a data product that becomes an invaluable bid preparation, analysis and feedback tool. Something that allows non-technical users to ask questions about the tenders coming out of the Irish Govt, identify tenders that we maybe should of bid for that didn't and ultimately provides a sales feedback loop that drives further commercial successes.\r\n\r\n## Data Pipeline & Data Sources\r\n\r\nThere are 7 data sources when it comes to updating the database, turning it into the commercial backbone of the Sales Process.\r\n\r\nThe pipeline contains the traditional Extract, Transform & Load stages BUT in fact it's more in-depth, thanks to the introduction of AI.\r\n\r\n1. ETL\r\n   - Scrape (extract)\r\n   - Parse (transform)\r\n   - Load\r\n\r\n2. AI Enrichment\r\n   - PDF Parsing with Ollama\r\n\r\n3. AI Analysis\r\n   - Bid recommendation with Ollama\r\n\r\n4. Business Updates\r\n   - Bid Submission\r\n   - Sales Updates\r\n   - Awards\r\n\r\nAlthough a \"load\" step exists in the first \"ETL\" stage we're in fact writing the data to the database (or files) as each step completes.\r\n\r\n## DB Schema & ERD\r\n\r\n```mermaid\r\nerDiagram\r\n    ETENDERS_CORE ||--|| ETENDERS_PDF : \"has\"\r\n    ETENDERS_CORE ||--|| BID_ANALYSIS : \"generates\"\r\n    ETENDERS_CORE ||--|| CPV_CHECKER : \"validates\"\r\n    ETENDERS_CORE ||--o{ SALES_UPDATES : \"receives\"\r\n    ETENDERS_CORE ||--o| BID_SUBMISSION : \"submits\"\r\n    ETENDERS_CORE ||--|| TENDER_NOTIFICATION : \"awarded\"\r\n    \r\n    ETENDERS_CORE {\r\n        string resource_id PK\r\n        int row_number\r\n        string title\r\n        string detail_url\r\n        string contracting_authority\r\n        string info\r\n        date date_published\r\n        datetime submission_deadline\r\n        string procedure\r\n        string status\r\n        string notice_pdf_url\r\n        date award_date\r\n        decimal estimated_value\r\n        int cycle\r\n        datetime date_published_parsed\r\n        datetime submission_deadline_parsed\r\n        datetime award_date_parsed\r\n        decimal estimated_value_numeric\r\n        int cycle_numeric\r\n        boolean has_pdf_url\r\n        boolean has_estimated_value\r\n        boolean is_open\r\n    }\r\n    \r\n    ETENDERS_PDF {\r\n        string resource_id PK,FK\r\n        string pdf_url\r\n        boolean pdf_parsed\r\n        text pdf_content_full_text\r\n    }\r\n    \r\n    BID_ANALYSIS {\r\n        string resource_id PK,FK\r\n        boolean should_bid\r\n        decimal confidence\r\n        text reasoning\r\n        text relevant_factors\r\n        decimal estimated_fit\r\n        datetime analyzed_at\r\n    }\r\n    \r\n    CPV_CHECKER {\r\n        string resource_id PK,FK\r\n        int cpv_count\r\n        string cpv_codes\r\n        text cpv_details\r\n        boolean has_validated_cpv\r\n    }\r\n    \r\n    SALES_UPDATES {\r\n        int id PK\r\n        string resource_id FK\r\n        date date\r\n        text sales_comment\r\n        string author\r\n    }\r\n    \r\n    BID_SUBMISSION {\r\n        string resource_id PK,FK\r\n        date submission_date\r\n        string bid_document\r\n        string author\r\n    }\r\n    \r\n    TENDER_NOTIFICATION {\r\n        string resource_id PK,FK\r\n        date notification_date\r\n        string winning_bidder\r\n        text bid_terms\r\n        text award_reasons\r\n    }\r\n```\r\n\r\nFigure 7: DB Schema and ERD\r\n\r\nThe diagram shows:\r\n\r\n- **1:1 relationships** between `ETENDERS_CORE` and `ETENDERS_PDF`, `BID_ANALYSIS`, `CPV_CHECKER`, and `TENDER_NOTIFICATION`\r\n- **1:Many** relationship with `SALES_UPDATES` (multiple progress updates per tender)\r\n- **1:0 or 1** relationship with `BID_SUBMISSION` (optional, but max one per tender)\r\n\r\nAll tables use `resource_id` as the linking key, with `SALES_UPDATES` having its own auto-increment ID as the primary key.\r\n\r\n## Data Product\r\n\r\n### Scalability & Performance\r\n\r\nThe pipeline adds up to 50 records (maximum) per day and Sales Team members add updates but this isn't anything near enough to stretch PostgreSQL on either scalability or performance.\r\n\r\nWe've indexed common query patterns (date ranges, status, value), which keeps performance well within acceptable limits. Additional optimization becomes relevant only at 100,000+ records or with real-time requirements - neither applies to our use case.\r\n\r\nWe are going nowhere _near_ postgresql's limits (www.postgresql.org, n.d)\r\n\r\n### Security\r\n\r\n**Who has access?**\r\nRole-Based Access Control (RBAC) splits access into groups and ensure \"least privilege\" (Microsoft RBAC, 2024)\r\n\r\nFor example we'd give Sales \"read\" access to everything but only \"write\" access to bid submissions fields and Management would get \"read all\" and so on.\r\n\r\nOnly authorized individuals have access using Azure AD integration with Single Sign-On (SSO) and mandatory Multi-Factor authentication (najshahid, 2025). Access should be auditable in the logs alongside other database activity.\r\n\r\n**Data Protection**\r\nAll data should be encrypted at rest and in transit using TLS 1.3 or higher with API keys and secrets held in secure storage like Azure Key Vault with automated key rotation built in.  (msmbaldwin, n.d.)\r\n\r\n**Application Security**\r\nInput validation prevents things like SQL injection attacks and XSS (cross site scripting) while rate limiting on any API's prevent miss-use.\r\n\r\n**Network Security**\r\nThe database should NOT be publically available on the web. If we _do_ happen to need this we should at least enable DDOS protection on any public IP/ports. Use Network Security Groups with a port/source whitelist and also put everything behind a NAT Gateway (asudbring, 2023)\r\n\r\n**Compliance & Governance**\r\nEnlist a third party carries out an annual independent security audit and there should be an incident response plan for responding to a possibl data breach.\r\n\r\nThe data can also be considered as 'internal only' and 'confidential' so this should inform our security architecture.\r\n\r\n### Regulatory Compliance\r\n\r\n**GDPR:**\r\nOnly the necessary minimum data should be collected and certainly not anything that could be considered personally identifiable information (Wikipedia Contributors, 2019)\r\nWe can use AI related services from cloud providers to purge public identifiable information (PII) and have the ability to remove any on request. (laujan, 2025)\r\n\r\n**Data Retention and Auditing:**\r\nMonthly point in time recovery with a years backups in archive (to reduce cost).\r\n\r\nActivity logging allows auditing of data access requests, logins and even performance behaviour.\r\n\r\n**Industry Standards:**\r\nWe need to document security controls like ISO 27001 (International Organization for Standardization, 2022) and any cloud compliance certificates i.e. ISO 27018 which is cloud specific (for, 2025)\r\n\r\n### Flexibility for future Use\r\n\r\n**MCP Server Integration:**\r\nModel Context Protocol implementation empowers end-users to ask natural language questions of the data, for example: - \"Show me all the healthcare tenders over €1m in value in the last 12 months\".\r\n\r\nThis also enables other AI Assistants to return structured responses when they decide to get this data to answer a related question.\r\n\r\n**Predictive Analytics and AI:**\r\nOnce enough tender and response data exist this can be used in the ML training to return an overall probability of a successful bid based on historical patterns.\r\n\r\nAnother application might be taking the previously submitted winning bids and having AI produce an auto-generated first draft for new responses that takes context from historical successes.\r\n\r\n**Technology Agnostic Design:**\r\nA more generic architectural approach like containerisation (Powell, 2021) might be useful for it's ability to be deployed on any cloud, or on-prem.\r\n\r\nThis means we won't get painted into a corner when going from building the solution (the Dev part) to deploying and running it i.e. the \"Ops\" part.\r\n\r\n### Best practices\r\n\r\nThere are a LOT of sources and influences on best practices when it comes to database design, deployment, software engineering and AI/ML.\r\n\r\n**Data Engineering:**\r\nIf we're going to host this solution in the cloud (again) we can look at the Microsoft or AWS well architected framework guides (PageWriter-MSFT, 2023) which inform security, reliability, operational scalability and cost-optimisation at the 'cloud' level.\r\n\r\nThese also link to DevOps practices like deploying everything using infrastructure as code tools (bicep, terraform, pulumi), using version control for scripts, automated testing and CI/CD pipelines.\r\n\r\n**Software Development:**\r\nThis covers basic things like clean code principles (Codacy, 2023), having meaningful code reviews, deployment gates and adhering to the twelve factor app methodology as much as possible (Wigging, 2017).\r\n\r\n**AI/ML best practices:**\r\nWe must follow responsible AI principals (mesameki, 2024) with a human in the loop to check any decision making and ensure accountability.\r\n\r\n<!--\r\n**References:**\r\n- Azure Architecture Center: Reference architectures for data analytics solutions\r\n- Google SRE Book: Site Reliability Engineering principles\r\n- DAMA-DMBOK: Data Management Body of Knowledge for data governance\r\n- OWASP: Web application security standards and testing guidelines\r\n-->\r\n\r\n<!--\r\n● Understand the specific business problems to be addressed and identify all necessary data sources.\r\n● Plan and define the data solution, including any ETL processes or data flows. Consider data formats, sizes, and frequency of updates in the design.\r\n● Choose an appropriate data solution (see list below), focusing on compatibility and scalability. This may be a low-code, no-code, or a coded solution, depending on business needs.\r\n● Develop your data product, conduct comprehensive testing, and automate with scheduling.\r\n● Focus on scalability, performance optimization, security, and regulatory compliance.\r\n● Ensure flexibility for future changes and adherence to best practices.\r\n-->\r\n\r\n<!-- Milestone 2\r\n## Design and develop a scalable, secure ETL pipeline proof of concept that adheres to industry best practices.\r\n  - Stakeholder Identification and Needs Analysis\r\n  - Identify key stakeholders involved in the data engineering process and detail their roles.\r\n  - Conduct a needs analysis based on stakeholder feedback, categorizing primary requirements and challenges.\r\n  - Prioritize these needs in alignment with business goals and outline high-level requirements for a data solution.\r\n## ETL Process Requirements and Solution Selection\r\n  - Specify the ETL process requirements, focusing on data types, flow frequency, volume, and data quality standards.\r\n  - Compare and evaluate at least two ETL tools or frameworks (e.g., Apache Airflow vs. AWS Glue) for their alignment with organizational requirements.\r\n  - Justify the selection based on criteria like performance, scalability, security, and regulatory compliance.\r\n-->\r\n",
  "5": "# Impact Assessment and Recommendations for Rollout <!-- 800 words -->\r\n\r\nLet's not forget that SALES generate REVENUE so ultimately our goal and the measure of our success is that we've contributed in a meaningful way to increased billing.\r\n\r\nThis approach delivers a secure, monitorable and maintainable data product while leaving room for additional features and valuable integrations to other data sources.\r\n\r\n## Business Wins\r\n\r\nIn addition to the reduction in Sales admin the business can display key business intelligence information from the data.\r\n\r\n![eTenders Intelligence Dashboard](images/dashboard_main.png)\r\nFigure 18: eTenders Intelligence Dashboard\r\n\r\nShows the number of bids AI has recommended should be responded to, the total value of these bids (€72 million) while making it clear that 85% of tenders that are published aren't worth responding to!\r\n\r\n![eTenders Top Opportinities by Value](images/dashboard_top_opportunities.png)\r\nFigure 19: eTenders Top Opportunities by Value\r\n\r\nShows the top opportunities by value as well as the overall distribution of AI recommended bids by value. Perhaps un-surprisingly IT consultancy related tenders tend to cluster > €500K range.\r\n\r\nGiven this visualization business stakeholders should be able to get an 'at a glance' appreciation for the value of pulling this data into a queriable database.\r\n\r\nHaving Sales Team members append update records to tenders in progress would allow them to monitor additional sales related KPI's and drive sales improvements:\r\n\r\n- When was the last time a bid was updated?\r\n- How often are customers contacted?\r\n- Percentage of won/lost bids?\r\n- How long does it take to respond to a tender?\r\n- How many points of contact happen before a bid is won?\r\n\r\nAll of these metrics get added to the tender records as they progress to help answer the question \"What does our process of winning a tender look like?\"\r\n\r\nAs we see from the data, the cost of missing a bid or losing an opportunity is already very high. We can quible over implementation costs but if this effort tips the chance of winning even a portion of the €70 million then it's worth doing.\r\n\r\n## Implementation and Rollout Plan\r\n\r\nIn order to de-risk the development and deployment of the solution we can take a phased approach\r\n\r\n<!-- markdownlint-disable MD033 -->\r\n<style>\r\n  /* Default for dark themes - white text */\r\n  .mermaid text {\r\n    fill: white !important;\r\n  }\r\n  .mermaid .taskText,\r\n  .mermaid .sectionTitle,\r\n  .mermaid .grid text,\r\n  .mermaid .tickText,\r\n  .mermaid .titleText,\r\n  .mermaid .labelText,\r\n  .mermaid .loopText,\r\n  .mermaid .actor text {\r\n    fill: white !important;\r\n  }\r\n  \r\n  /* Handle mdBook light and rust themes - black text */\r\n  html.light .mermaid text,\r\n  html.light .mermaid .taskText,\r\n  html.light .mermaid .sectionTitle,\r\n  html.light .mermaid .grid text,\r\n  html.light .mermaid .tickText,\r\n  html.light .mermaid .titleText,\r\n  html.light .mermaid .labelText,\r\n  html.light .mermaid .loopText,\r\n  html.light .mermaid .actor text,\r\n  html.rust .mermaid text,\r\n  html.rust .mermaid .taskText,\r\n  html.rust .mermaid .sectionTitle,\r\n  html.rust .mermaid .grid text,\r\n  html.rust .mermaid .tickText,\r\n  html.rust .mermaid .titleText,\r\n  html.rust .mermaid .labelText,\r\n  html.rust .mermaid .loopText,\r\n  html.rust .mermaid .actor text {\r\n    fill: black !important;\r\n  }\r\n  \r\n  /* Ensure dark themes have white text */\r\n  html.navy .mermaid text,\r\n  html.navy .mermaid .taskText,\r\n  html.navy .mermaid .sectionTitle,\r\n  html.navy .mermaid .grid text,\r\n  html.navy .mermaid .tickText,\r\n  html.navy .mermaid .titleText,\r\n  html.navy .mermaid .labelText,\r\n  html.navy .mermaid .loopText,\r\n  html.navy .mermaid .actor text,\r\n  html.ayu .mermaid text,\r\n  html.ayu .mermaid .taskText,\r\n  html.ayu .mermaid .sectionTitle,\r\n  html.ayu .mermaid .grid text,\r\n  html.ayu .mermaid .tickText,\r\n  html.ayu .mermaid .titleText,\r\n  html.ayu .mermaid .labelText,\r\n  html.ayu .mermaid .loopText,\r\n  html.ayu .mermaid .actor text,\r\n  html.coal .mermaid text,\r\n  html.coal .mermaid .taskText,\r\n  html.coal .mermaid .sectionTitle,\r\n  html.coal .mermaid .grid text,\r\n  html.coal .mermaid .tickText,\r\n  html.coal .mermaid .titleText,\r\n  html.coal .mermaid .labelText,\r\n  html.coal .mermaid .loopText,\r\n  html.coal .mermaid .actor text {\r\n    fill: white !important;\r\n  }\r\n  \r\n  /* Ensure links and other specific elements have correct colors in light themes */\r\n  html.light .mermaid .flowchart-link,\r\n  html.rust .mermaid .flowchart-link {\r\n    stroke: #333 !important;\r\n  }\r\n  \r\n  /* Ensure links have correct colors in dark themes */\r\n  html.navy .mermaid .flowchart-link,\r\n  html.ayu .mermaid .flowchart-link,\r\n  html.coal .mermaid .flowchart-link {\r\n    stroke: #ccc !important;\r\n  }\r\n  \r\n  /* Additional styles for better visibility in all themes */\r\n  .mermaid .grid path {\r\n    stroke-opacity: 0.5;\r\n  .mermaid .today {\r\n    stroke-width: 2px;\r\n  }\r\n</style>\r\n<!-- markdownlint-enable MD033 -->\r\n\r\n```mermaid\r\ngantt\r\n    title Prototype\r\n    dateFormat YYYY-MM-DD\r\n    axisFormat %b '%y\r\n    tickInterval 1month\r\n\r\n    section Phase 1: Prototype\r\n    Requirements Gathering :a1, 2025-07-01, 2w\r\n    Prototype              :a2, after a1, 3w\r\n    Stakeholder Review :crit, a3, after a2, 1w\r\n    Go/No-Go Decision       :crit, milestone, a4, after a3, 0d\r\n\r\n    section Phase 2: Development & Deployment\r\n    Sprint 1 :a5, after a4, 2w\r\n    Sprint 2 :a6, after a5, 2w\r\n    Container deployment using IAC :a7, after a6, 1w\r\n    Security Audit :crit, a8, after a7, 1w\r\n\r\n    section Phase 3: Business Validation\r\n    User Acceptance Testing :a9, after a8, 1w\r\n    User Feedback :a10, after a9, 1w\r\n    Performance Monitoring :a11, after a10, 1w\r\n    Production Live :milestone, after a11, 0d \r\n```\r\n\r\nFigure 20: Phased Delivery Plan\r\n\r\n### Phase 1: Prototype (6w)\r\n\r\nThe goal is to validate the technical feasibility as well as ensure all stakeholder feedback has been considered. We can use the following KPI's to ensure all the requirements have been met:\r\n\r\n- Majority (>80%?) of stakeholders approve moving forwards\r\n- Technically feasible in that eTenders can be obtained, stored and processed\r\n- Prototype can be created with a set of minimum features (MVP)\r\n\r\n#### Phase 1: Contingency\r\n\r\nIf the PoC proves challenging then pivot to using a different datasource, running everything manually for a while or deploying as something other than a container. Azure Functions might be an alternative.\r\n\r\n### Phase 2: Development and Deployment (6w)\r\n\r\nPhase 2 goal is to develop and deploy the solution securely in the cloud paying attention to the following measures to validate success:\r\n\r\n- Test coverage that ensures code quality\r\n- Successful deployment to the Azure cloud environment\r\n- Resources deployed pass Microsoft's cloud security checks\r\n\r\n#### Phase 2: Contingency\r\n\r\nDeployment might be delayed to allow security issues to be patched or possibly the run-rate budget might need to increase to include higher cost protections like Nat Gateways and Azure AD for access.\r\n\r\n### Phase 3: Business Validation (3w)\r\n\r\nThe goal here is to be able to demonstrate business value and validate user adoption in the short term. Longer term measures will only come to light after several bid cycles have been completed.\r\n\r\n#### Short term business measures\r\n\r\n- High (>80%) uptake of using the tool by the Sales Team\r\n- Reduction in reported sales admin (time) by Sales Team\r\n- User satisfaction survey reports improvement (anonymous Sales Team survey)\r\n\r\n#### Long term business measures\r\n\r\n- Overall increase in bids won (revenue) over a 6 - 12 month period\r\n- Identify more opportunities to bid on\r\n- Additional features and integrations are requested by the Sales Team as this proves the product is becoming more useful to the business overall\r\n- Additional requests for analytics and dashboards from Sales Management which again shows interest and buy-in\r\n\r\n#### Phase 3 Contingency\r\n\r\nIf users struggle then additional training would have to be considered and for some users to be selected to champion the use of this tool as a replacement for out-dated manual processes. As with all implementations buy in and support from Management is essential for success.\r\n\r\n## Summary\r\n\r\nThe dashboard clearly shows the value of having a well architected data product that surfaces business value and can drive commercial success.\r\n\r\nTaking a phased approach to delivery keeps all contributors on track and leaves scope for further improvements later.\r\n\r\n<!--\r\n● Set measurable goals for your data product, aiming for improvements in data processing speed, accuracy, and resource efficiency.\r\n● Monitor improvements in processing time, data quality, resource utilisation, scalability, and error rates.\r\n● Assess impacts on user productivity, business decision-making, cost savings, and revenue. Collect user feedback to gauge satisfaction and ease of use.\r\n● Compile findings into a comprehensive report with visualisations for clarity.\r\n-->\r\n\r\n<!--\r\n\r\nDevelops a value analysis showing the potential impact of a data-driven solution and distinctly justify your approach to stakeholders (K14)\r\n\r\nRUBRIC - B\r\n\r\nDevelops a value analysis that quantifies the potential impact of a data-driven solution across multiple business metrics. Includes thorough analysis of stakeholder needs from key departments. Provides a clear implementation plan with defined milestones.\r\n\r\nRUBRIC - A\r\n\r\nDevelops a multi-faceted value analysis that quantifies short-term and long term impacts of a data-driven solution, including ROI projections. Incorporating analysis of stakeholder needs across all levels of the organisation. Provides a detailed implementation plan with phased rollout strategy, specific KPI's for each phase and contingency plans.\r\n\r\n-->\r\n"
}
